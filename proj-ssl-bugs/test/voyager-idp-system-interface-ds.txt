Voyager Intrusion Detection and Prevention(IDP) 
System Interface Design Specification

Copyright (C) 2006, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential
information of Juniper Networks, Inc. and must not be distributed
outside of the company without the permission of Juniper Networks
engineering.

Contributors: 
Raju Manthena   raju@juniper.net
Amit Shah       amitshah@juniper.net

Version 
0.1  First Draft 

1.  INTRODUCTION

This design specification describes Voyager IDP system interface. 

This document primarily focuses on IDP native support on Voyager 2.0(IDP software
does not require any co-processing modules). Alternately, to run IDP on ASPIM
(Nike, co-processing module) is being considered.

Voyager 2.0 IDP Functional Specifications:
http://p-wiki1.juniper.net/twiki/pub/Voyager2/FunctionalSpecs

Release Line Items: 4758
https://deepthought.juniper.net/app/do/showView?taskCode=all&
             record_number=4785&tableName=RLI
PR: 94858
http://gnats/cgi-bin/gnats?debug=&database=default&cmd=view&pr=94858

2.  FUNCTIONALITY

Goals:

Non-goals:

Assumptions:

TBD

3.  CAVEATS

4.  OTHER REQUIREMENTS

4.1 OS Requirement

IDP places minimal requirement on its operating environment and its
primitives. IDP needs a way to dynamically allocate memory during run-time,
file system access, dynamic module loading support, locking and synchronization
primitives, RAM disk space of ~256MB and IPC across real-time, PFE unix and RE.

4.2 Platform Requirement

IDP places the following interface requirements on Voyager base system
    1)  for holding packet in memory (essentially reference count)
    2)  Dynamic loadable module support for dynamic decoder update

4.3 Hardware Requirement

N/A

4.4 Security Engineering

N/A

5.  IMPLEMENTATION DETAILS


5.1     Module Diagram

RTCore/BSD on Voyager provides a scheduling environment that allow packet
forwarding threads and other RT threads to be run in real-time environment.
JunOS control plane runs a single RTCore (idle) thread.

Voyager-IDP design will take the following considerations into account.

1) Ensure the data that is generated by the packet processing plane (events,
   profiling data)and ship it to the control plane for export to management
   systems or storage device.
2) Make the design flexible so that the control plane (RE) and the Data plane
   components may not necessarily run on the same CPU.
3) Also take into consideration that IDP processing will eventually
   be distributed in multiple cards (like NIKE for Voyager and SPCs for Viking).
   The RE functionality may not be replicated and so configuration and policy
   data need to be conveyed from the RE to the various cards and similarly
   event/profiler data from multiple sources need to be correlated in the RE.
4) Provide/leverage some form of IPC mechanism between the RE and the processing
   cards. It is a goal to have a multicast IPC mechanism so that configuration
   data can be sent to a group address that would include for example all
   IDP modules.

The below module diagram illustrates how different modules are layered and how
do they interact.


<---- Control Plane -------> <------   PFE Unix  -----> <-------- PFE RT --------------->


                                  ProfMan                   Prof-RT                Flowd-RT
                                     |                        |                        |
                                     |                        | pkts/events +-----+    |                  
                                     |                        |<----------->|     |    |   
                                     |<----------|RTUX CH |---|             | SM  |    |
                         +-----+     |                        |      Idp-Rt | API |<-->|
         +---------+     |     |     |   IdpMan   +------+    |<-FIFO- |    |     |    |
  +----->|profilerd|     |     |     |     |      |      |    (L7-ctx) |<-->|     |    |
  |   +- |         |<-+  | USP |<--->|     |      | idp  |             |    +-----+    |
  |   |  +---------+  |  | IPC |     |     |=====>|policy|<============|               |
  |   |               |  | --  |           |      |      |             |
  |   |  +---------+  +--| Dev |           |      +------+             |    
  |   +- |idphwd   |<----| API |<--------->|                           |    
  +-- | >|         |  +--|     |           |      +------+             |    
  |   |  +---------+  |  |     |           |=====>|      |<============|    
  |   |               |  +-----+           |      | idp  |             |
  |   |  +---------+  |                    |      | flow |             |    
  |   |  |idpd     |<-+  +-----+    PfeMan |      |table |             |
  +-- | >|         |<----|  S  |      |    |      +------+             |
  |   |  +---------+     |  S  |      |<-->|                           |
  |   |                  |  A  |<---->|                                |
  |   |                  |  M  |      |                                |
  |   |DaemonLogs        +-----+                  +------+             |
  |   |  +--------+                               | Log  |Security Logs|
  |   +->|        |<------------------------------| API  |<------------|
  |      | eventd |---+                           |      |               
  |      +--------+   |                           +------+               
  |                   |
  |config+---------+  |    
  +----- |  mgd    |  |
 XMLRPC->|config-db|  |
         +---------+  |   
                      | 
         +--------+   |     
 Logs<-- |syslogd |<--+ 
         +--------+        
+-----------------------------------------------------+
|                JUNOS BSD Kernel                     |
+-----------------------------------------------------+--------------------------------+
|                                   RTCORE/BSD                                         |
+--------------------------------------------------------------------------------------+

IDP will run as separate RT thread for two reasons viz. modularity and to
support fairness in burning CPU cycles between IDP and FW(and other services).
This thread will access IDP policy, IDP session/flow tables  and (binary)
attack objects.

IDP RT thread receives packets through Session Manager. This API will be
designed to take care of platform (Voyager, Viking etc.) and design dependencies
(single thread vs separate thread) and provide a consistent function call layer.

IDP Manager thread is a helper thread to load/unload/verify policy and sensor
configuration/settings. It receives the input binary policy, creates tries and
policy objects etc and notifies IDP RT thread about the new policy. To avoid
locks, IDP manager thread will notify IDP RT thread through RT-Unix channel
(Alternately Flowd-RT thread can have a stub to relay this message to IDP RT
thread).

On receiving the notification, IDP RT thread will take the new policy reference.
Multiple versions of the policies can coexist. New flows take the latest policy
and old flows continue to use old policy. Old policies will be unloaded when
their reference count becomes zero.

PFE manager and idp manager threads are in PFE Unix part of the PSDD (and other
threads on PFE unix), they need cooperative scheduling.

Profiler runs as low priority RT thread. For network profiling, profiler will be interfaced
with Session Manager. For profiling application contexts, IDP RT thread will send the 
contexts profiled to Profiler RT thread.

Profiler Manager Thread on the PFE unix part will get the profiler data to be sent over
to Profiler daemon on RE. It used a RTUX channel to get contexts from IDP-RT thread and 
USP_IPC channel to transfer this data across to the RE world. 

IDP Control Plane will leverage the existing JunOS command and configuration
infrastructure.

IDP policy manager (idpd) will support policy compilation and handle
configuration requests handling. Profilerd receives the network and application
contexts collected by the data plane.

Status Monitor (idphwd) will be the watch dog for status collection and
population.

Profiler Daemon (profilerd) maintains flow and context information in several tables.
It collects, aggregates and transfers profiled information to NSM.

The SSAM library provides a common state management interface for both intra
and inter box state exchanges. The state management functionality would be
available to both control and data plane components. Currently, we have not
identified any object that need to be kept in SSAM.

Idpd and other idp control plane daemons use the USP_IPC to send configuration
and (compiled) policy to data plane.

Data plane logs (attack and event logs) will leverage system wide logging
infrastructure. A new logging infrastructure is being designed for
Voyager/viking.  Data plane logs will eventually sink to eventd 
on control plane.  Control plane daemon logs will sink into eventd.  

CLI, JWeb and any JunOScript client will use XML RPC to communicate with
Management Daemon(mgd). Mgd is the heart of the configuration and command
infrastructure.

idpd will use JUNOS opscript support to change junos-config-db for policy
template support. idpd will maintain it's own dynamic database (idpd-db) to
store predefined attacks, groups and policy templates. 

5.2 OS Service mapping and operating environment

As the IDP code is modular, has platform dependent (<5%) and platform 
independent code. The platform dependent code utilizes basic services
like locking, timers, dynamic memory allocation. Voyager base OS also
need to provide a mechanism to dynamically load and unload part of the
code without disrupting the transiting traffic flow. 

Voyager uses RTCore/BSD running JunOS kernel as the idle thread.  It does not
matter whether we use a kernel thread or a PSDD process for IDP functionality;
IDP will be guaranteed hard real-time response.  RT kernel thread vs PSDD
process is similar in all respects except for the programming part (kernel
functions or system calls respectively).

IDP RT thread will be part of Flowd process. IDP uses Session Manager API,
Packet Mapping API and Memory API. Actual method of communication will be
implemented these APIs and remains transparent for the communicating modules.
Having these APIs will ease the way we build IDP data plane for various platforms.
Any platform dependencies will be implemented with in these API and other
IDP processing code can be ported straight forward manner.

The required OS primitives include locks, dynamic memory allocation, packet
layout & structure etc.

Memory allocation/free:
For memory allocation/free routines, in kernel/kabs/src/sc_malloc.c, add 

#elif defined (SC_JUNOS)
         #include 
        #include 
        #define do_malloc(size)       kmalloc(size)
        #define do_malloc_small(size) do_malloc(size)
        #define do_malloc_large(size) do_malloc(size)
        #define do_free(ptr, size)    free(ptr)
        #define do_free_small(ptr, size) do_free(ptr, size)
        #define do_free_large(ptr, size) do_free(ptr, size)
#else

For data plane IDP, ensure we get appropriate headers/API from PFE ukernel
includes. For other control plane daemons, ensure right system includes are
grabbed in the makefile. 

Timers:
IDP timer event dispatch is triggered by packet arrival. IDP internally
maintains the timer hash list which will be polled in the packet processing
path. IDP data plane does not require timer support from underlying OS;
its enough if we get the current time of the system. 

Implement sc_ktime_now() for JUNOS,  in kernel/ds/include/sc_ktimer.h

static inline sc_ktime_t 
sc_ktime_now()
{
#elif defined(SC_JUNOS)
        return (sc_ktime_t) (rt_gettime() * NSEC_PER_SEC);
}

Synchronization:
The synchronization routines for Data Plane RT will be void. We will keep
this interface to add Voyager data plane locking mechanism when it supports
multicore.  For locks in PFE, we can use PFE mutex_lock() mechanism.

In kernel/kabs/include/sc_kmutex.h, add a case for SC_JUNOS and appropriately
write the spin_kmutex_xxx() defines

5.3 Packet Mapping API

Since the protocol decoders and content processing is completely abstracted
from the underlying packet reception and representation, there will be a mapping
layer/Packet Mapping API to do the conversion from sm_packet_s to
sc_kpacket_t.<> 

Within IDP, there are well defined packet data-access interfaces through which
the packet content is accessed.  As the original packet (mbuf/sm_packet_s) is 
never modified with in the IDP, we don't have to do anything while handing off
the packet back to Voyager Session Manager. 

sc_kpacket_t  
*sc_kpacket_attach_ip(void *buf, const struct sc_vcircuit_s *vcp, 
                        sc_nic_dir_t dir)
{
/* do some sanity checks on the passed in mbuf, if necessary */
/* allocate kpp from packet pool */
/* Populate kpp */
}

Underlying packet reference counting is a must. 

Packet data is accessed via sc_kpacket library. Library interface is defined in 
net/include/sc_kpacket.h
net/include/sc_kpacket_io.h
net/include/sc_kpacket_util.h

sc_kpacket.h and sc_kpacket_io.h have interface for platform dependent part. 
This part is implemented for Voyager with packet(mbuf) as underlying OS packet. 
Following points need to be considered while writing these wrapper function in
packet mapping API:
    *   mbuf may not be linear always. It can come as a chain of mbufs. But most
        of the time the data IDP is interested in will be in one mbuf cluster
        as a contiguous memory.
    *   fwdd puts some per-packet information in front of actual packet. 
        IDP we should use mtod() to get the start of packet (IP header).
        Don't use M_DATA_START()
 
Following functionalities are platform dependent -
    *   Generating IP/ARP/STP packet or frame and sending them through a 
        specified interface - IDP will use Session Manager API
    *   Sending IP/ARP up the protocol stack.
        Not required for Voyager
    *   Get/Set packet data.
   Should take care of mbuf chain.
    *   Packet cloning
        m_copym() is used to copy mbuf header and increment ref-count of
        cluster. Packet API should include this functionality

5.4 IDP Memory Manager

TBD

5.5 IDP Engine and decoder updates

IDP data plane components can be logically divided into infrastructure engine
(Q-modules, OS dependent interfaces) components and detector components
(Protocol decoders - part of IDS Q-module). The infrastructure components do
not need frequent updates (perhaps requires a release to do such change), the
detector components may need to change, for e.g. to add new contexts to a
protocol or fix vulnerabilities within the IDP itself.  

It desired to have multiple versions of detectors in memory (old sessions
will use old detector and new sessions can use updated detector).

TBD

5.6 Packet Processing

Session Manager module hands off a packet for IDP processing after necessary 
checks to see if this packet needs IDP processing. Service Chaining will be
hidden by Session Manager.

The IDP packet processing thread runs in a tight loop, polling for new packets
that may be queued by the Session Manager for IDP processing. If a packet is
not available, we yield control of the CPU. If a packet is available for
processing, we take it off the queue and perform IDP operations on it.

The pseudo-code for this operation looks like this:

/* IDP Packet processing thread */

while (1) {
   if (isempty(IDP_Q)) {
         yield();
         continue;
   }

   /* Process packet */
}

In the RTCore/BSD environment, the scheduler is implemented such that even within
RT threads, there are CPU budgets. A realtime thread may be pre-empted to let other
realtime threads run. This requires each realtime thread to be preempt-safe. The
IDP processing has been running on a non-preempt, single-CPU system till now.
There might be some data structures or code paths that would need reordering to make
them preempt-safe. These changes can be found during the effort to make the IDP
code SMP-safe as part of the Viking effort.

The "subscriber" and Q-modules from the standalone IDP implementation will be
retained. However, we will mask these details from the outside world. In the
current phase, the subscriber will consist of a fixed set of Q-modules.

The Q-modules form a chain of processing modules through which a packet is
passed. These modules each perform some operations on the packet itself or
on the flow the packet has arrived on. Examples of Q-modules are the TCP
reassembler Q-module and the IDS Q-module.

The Q-modules form a chain or a queue of modules, hence they're labelled
Q-modules. If the IDS Q-module is placed after the TCP reassembly Q-module,
it need not be bothered about out-of-order data. It will be assured of getting
reassembled TCP data.

The Q-modules that worked on the IP layer in the standalone IDP and ISG products
will not be used. IP fragmentation, reassembly will be handled by the base
system itself. IDP processing will take care of flow, TCP reassembly, 
syn defender, seq-ack-translator, backdoor, ids, portfaker (network honeypot),
tsig.

Each Q-module can take one of three actions on the packet: 
        a) drop b) hold c) continue

This result is passed on to the Session Manager. For "hold", we shall increment
the refcount for the packet, so that it is not freed up by the base system. It
will be freed only when the refcount goes to 0. We may need to hold packets
for cases like TCP reassembly or stream signature matching.

There is a timeout associated for packets that are in 'hold' state, so that
we don't end up holding on to packets for a long time and starving
the system of memory.

The first Q-module to be invoked will be the flow Q-module. This module sets up
the flow information and associations with sessions. A flow connects two logical
end-points of a connection. It refers to a uni-directional packet flow. That, 
together with the other direction of the packet flow form two flows of a 
session. A session may have more flows, depending on the protocol. More details
on flow are available in the flow manager design specification.

When a new flow is created, each Q-module's init function is called. If the
Q-module is interested in processing that flow, it sets up its data structures,
allocates some memory and lets the caller know that it's interested in
examining future packets on the flow. Otherwise, that Q-module is not
invoked for subsequent packets on the flow.

Layer 7 classification is done on the first packet of a flow. This is done based
on the destination port. A mapping of port to service is available as part of
code, in the policy and dynamically via the Application Identification feature.
The static mapping can be overridden by the mapping that can be provided via
the policy. 


5.7  TCP Reassembly

TCP reassembly needed for handling stateful packet inspection for tcp out of
order segments. IDP may need to hold (essentially reference count) packets in
reassembly chain. 

As IDP might hold packets, Voyager base system must take care of delay/bandwidth
requirements of transiting traffic. 

TCP reassembly functionality in the Flow processing is subjected application
specific requirement.  Voyager FW does not do full TCP reassembly. IDP will
not rely on this hence does reassembly of TCP stream on its own, before applying
IDP processing. 

TBD

5.8 Device Wrapper  

Please refer IPC/device wrapper design spec on cvs.juniper.net 
sw-projects/usp/specs/designspecs/idp/voyager-idp-devicewrapper-ds.txt

5.9 PFE threads

TBD


6.  PERFORMANCE

Currently, Voyager platforms support 2GB of main memory. We may not be able
to achieve target throughput when larger number of sessions supported, if the
memory limitation is not alleviated.

6.1.  Performance Related Resources

TBD

6.2.  Target Performance

7.  COMPATIBILITY ISSUES

8.  SECURITY ISSUES

9.  Graceful RE Switchover (GRES), Hobson Impact

10.  NOTES

11.  GLOSSARY

12.  REVIEW COMMENTS
